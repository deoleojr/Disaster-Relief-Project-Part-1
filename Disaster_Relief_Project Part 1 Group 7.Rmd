
---
title: "Disaster Relief Project Part 1"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
authors:
  - name: "Leonce, Emmanuel D (fyb7sx)"
  - name: "Medal, Lionel (djz6nn)"
  - name: "Ontiveros, Victor Alberto (qfw3cr)"
    group: "7"
---

```{r hide-code, echo=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

## Introduction

In the wake of the devastating earthquake that struck Haiti in 2010, countless individuals were displaced, leaving them without shelter, food, or water. The aftermath presented significant challenges for rescue operations, particularly in locating those in need of assistance. With communication lines down and infrastructure severely damaged, the ability to quickly and accurately identify the locations of displaced persons became a critical priority. 

One innovative solution emerged from the efforts of the Rochester Institute of Technology, which involved collecting high-resolution geo-referenced imagery from aircraft flying over the affected areas. It was observed that many displaced individuals used blue tarps to create temporary shelters, making these tarps a crucial indicator of where aid was needed. However, the sheer volume of imagery collected each day made it impractical for human operators to manually search for these tarps and communicate their locations to rescue teams in a timely manner. 

To address this problem, data-mining algorithms offer a promising approach. By leveraging the power of machine learning, it is possible to automate the process of scanning the imagery, identifying blue tarps, and pinpointing the locations of displaced persons. This project aims to harness such algorithms to enhance the efficiency and accuracy of disaster relief efforts. 



## Data Exploration and Preprocessing

### Data Summary
```{r data-summary, echo=FALSE, message=FALSE, warning=FALSE}
# Load necessary libraries
# install.packages("MLmetrics")
library(MLmetrics)
library(tidyverse)
library(caret)
library(dplyr)
library(doParallel)
library(ggplot2)
library(yardstick)
library(pROC)
library(GGally)

# Set up parallel processing
cl <- makeCluster(detectCores() - 1)  # Using one less core to avoid overloading the system
registerDoParallel(cl)

# Load the training data
haiti_data <- read.csv('https://gedeck.github.io/DS-6030/project/HaitiPixels.csv')

# head of the data
head(haiti_data)

# structure of the data frame
str(haiti_data)

# Summary statistics
summary(haiti_data)

```

### Class Distribution
```{r class-distribution, echo=FALSE}
# Check the distribution of the 'Class' variable
table(haiti_data$Class)
```

### Visualizations
```{r visualizations, echo=FALSE, fig.height=14, fig.width=12}
# Histograms for each predictor
p1 <- ggplot(haiti_data, aes(x = Blue)) + geom_histogram(binwidth = 10, fill = "blue", alpha = 0.7)
p2 <- ggplot(haiti_data, aes(x = Green)) + geom_histogram(binwidth = 10, fill = "green", alpha = 0.7)
p3 <- ggplot(haiti_data, aes(x = Red)) + geom_histogram(binwidth = 10, fill = "red", alpha = 0.7)

# Display histograms
gridExtra::grid.arrange(p1, p2, p3, nrow = 3)

# Scatter plot matrix to visualize correlations
ggpairs(haiti_data, aes(color = Class))

# Calculate and display the correlation matrix
correlation_matrix <- cor(haiti_data[, c("Red", "Green", "Blue")])

# Correlation Matrix
print(correlation_matrix)


```

### Data Transformation
```{r data-transformation, echo=FALSE}
# Convert 'Class' to binary outcome variable and ensure valid variable names
haiti_data <- haiti_data %>%
  mutate(Class = ifelse(Class == 'Blue Tarp', 'BlueTarp', 'NonBlueTarp'),
         Class = as.factor(Class))

# Check the distribution again after transformation
table(haiti_data$Class)

# Ensure both classes are present
if (!all(c("BlueTarp", "NonBlueTarp") %in% haiti_data$Class)) {
  # Balance the data by oversampling the minority class
  haiti_data_balanced <- haiti_data %>%
    group_by(Class) %>%
    mutate(n = n()) %>%
    ungroup() %>%
    mutate(sampled = ifelse(n == min(n), TRUE, FALSE)) %>%
    filter(sampled | sample.int(n(), nrow(haiti_data) / 2))

  # Remove helper columns
  haiti_data_balanced <- haiti_data_balanced %>% select(-n, -sampled)
  
  haiti_data <- haiti_data_balanced
}

# Normalize predictors
predictors <- dplyr::select(haiti_data, Red, Green, Blue)
predictors <- as.data.frame(scale(predictors))
response <- haiti_data$Class
```

## Model Development

### Model Training
```{r model-training, echo=FALSE, message=FALSE}
# Create a training control object for cross-validation
train_control <- trainControl(method = "cv", 
                              number = 10, 
                              classProbs = TRUE, 
                              summaryFunction = multiClassSummary, 
                              savePredictions = TRUE)

# Combine predictors and response for model training
training_data <- cbind(predictors, response)

# Ensure the response variable is a factor with correct levels
training_data$response <- factor(training_data$response, levels = c("BlueTarp", "NonBlueTarp"))

# Train models using cross-validation

# Logistic Regression
logistic_model <- train(
  response ~ ., data = training_data,
  method = "glm", family = binomial, 
  trControl = train_control, 
  metric = "Accuracy"
)

# Linear Discriminant Analysis
lda_model <- train(
  response ~ ., data = training_data,
  method = "lda", 
  trControl = train_control, 
  metric = "Accuracy"
)

# Quadratic Discriminant Analysis
qda_model <- train(
  response ~ ., data = training_data,
  method = "qda", 
  trControl = train_control, 
  metric = "Accuracy"
)

# Print model performances
print(logistic_model)
print(lda_model)
print(qda_model)
```
```{r}
# Generate ROC curves
roc_logistic <- roc(logistic_model$pred$obs, logistic_model$pred$BlueTarp)
roc_lda <- roc(lda_model$pred$obs, lda_model$pred$BlueTarp)
roc_qda <- roc(qda_model$pred$obs, qda_model$pred$BlueTarp)

# Convert ROC objects to data frames
roc_logistic_df <- data.frame(
  sensitivity = roc_logistic$sensitivities,
  specificity = roc_logistic$specificities,
  model = 'Logistic Regression'
)
roc_lda_df <- data.frame(
  sensitivity = roc_lda$sensitivities,
  specificity = roc_lda$specificities,
  model = 'LDA'
)
roc_qda_df <- data.frame(
  sensitivity = roc_qda$sensitivities,
  specificity = roc_qda$specificities,
  model = 'QDA'
)

# Combine all ROC data frames
roc_combined_df <- bind_rows(roc_logistic_df, roc_lda_df, roc_qda_df)

# Plot ROC curves
roc_plot <- ggplot(roc_combined_df, aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() +
  labs(title = "ROC Curves", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal() +
  scale_color_manual(values = c("Logistic Regression" = "blue", "LDA" = "green", "QDA" = "red"))

print(roc_plot)



# Generate individual ROC plots for each model
roc_logistic_plot <- ggplot(roc_logistic_df, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "blue") +
  labs(title = "ROC Curve - Logistic Regression", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

roc_lda_plot <- ggplot(roc_lda_df, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "green") +
  labs(title = "ROC Curve - LDA", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

roc_qda_plot <- ggplot(roc_qda_df, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "red") +
  labs(title = "ROC Curve - QDA", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

# Print individual ROC plots
print(roc_logistic_plot)
print(roc_lda_plot)
print(roc_qda_plot)
```


```{r}
# Data representing the performance of the three models
data <- tibble::tibble(
  Model = c('Logistic Regression', 'Linear Discriminant Analysis', 'Quadratic Discriminant Analysis'),
  LogLoss = c(0.01406242, 0.06930316, 0.01696427),
  AUC = c(0.9984888, 0.9888762, 0.9981991),
  prAUC = c(0.8467475, 0.926855, 0.9754196),
  Accuracy = c(0.995272, 0.9839028, 0.9945763),
  Kappa = c(0.9204297, 0.7526515, 0.9054115),
  F1 = c(0.9228636, 0.7609553, 0.9081861),
  Sensitivity = c(0.8847632, 0.8011901, 0.8397552),
  Specificity = c(0.9989219, 0.9899378, 0.9996896),
  Pos_Pred_Value = c(0.9644403, 0.7251271, 0.9889958),
  Neg_Pred_Value = c(0.9962044, 0.9934111, 0.994734),
  Precision = c(0.9644403, 0.7251271, 0.9889958),
  Recall = c(0.8847632, 0.8011901, 0.8397552),
  Detection_Rate = c(0.02828859, 0.02561629, 0.02684965),
  Balanced_Accuracy = c(0.9418425, 0.8955639, 0.9197224)
)

# Display the dataframe
print(data)

```


## Model Evaluation

### Evaluation Metrics
```{r model-evaluation, echo=FALSE, message=FALSE, warning=FALSE}
# Load necessary libraries
library(tidyverse)
library(caret)
library(dplyr)
library(doParallel)
library(pROC)

# Set up parallel processing
cl <- makeCluster(detectCores() - 1)  # Using one less core to avoid overloading the system
registerDoParallel(cl)

# Load the training data
haiti_data <- read.csv('https://gedeck.github.io/DS-6030/project/HaitiPixels.csv')

# Convert 'Class' to binary outcome variable and ensure valid variable names
haiti_data <- haiti_data %>%
  mutate(Class = ifelse(Class == 'Blue Tarp', 'BlueTarp', 'NonBlueTarp'),
         Class = as.factor(Class))

# Normalize predictors
predictors <- dplyr::select(haiti_data, Red, Green, Blue)
predictors <- as.data.frame(scale(predictors))
response <- haiti_data$Class

# Create a training control object for cross-validation
train_control <- trainControl(method = "cv", 
                              number = 10, 
                              classProbs = TRUE, 
                              summaryFunction = twoClassSummary, 
                              savePredictions = TRUE)

# Combine predictors and response for model training
training_data <- cbind(predictors, response)

# Ensure the response variable is a factor with correct levels
training_data$response <- factor(training_data$response, levels = c("BlueTarp", "NonBlueTarp"))

# Train models using cross-validation

# Logistic Regression
logistic_model <- train(
  response ~ ., data = training_data,
  method = "glm", family = binomial, 
  trControl = train_control, 
  metric = "Accuracy"
)

# Linear Discriminant Analysis
lda_model <- train(
  response ~ ., data = training_data,
  method = "lda", 
  trControl = train_control, 
  metric = "Accuracy"
)

# Quadratic Discriminant Analysis
qda_model <- train(
  response ~ ., data = training_data,
  method = "qda", 
  trControl = train_control, 
  metric = "Accuracy"
)

# Print model performances
print(logistic_model)
print(lda_model)
print(qda_model)

# Function to process holdout data files
process_holdout_files <- function(file_path) {
  message(paste("Processing file:", file_path))
  data <- tryCatch({
    read.table(file_path, skip = 6, header = FALSE, fill = TRUE, col.names = paste0("V", 1:13))
  }, error = function(e) {
    message(paste("Error reading file:", file_path))
    return(NULL)
  })
  
  # Ensure the correct number of columns and set correct types
  if (!is.null(data)) {
    if (ncol(data) >= 10) {
      data <- data[, 1:10]
      colnames(data) <- c('ID', 'X', 'Y', 'Map_X', 'Map_Y', 'Lat', 'Lon', 'Red', 'Green', 'Blue')
      # Ensure all columns have the correct types
      data <- data %>%
        mutate(
          ID = as.character(ID),
          X = as.numeric(X),
          Y = as.numeric(Y),
          Map_X = as.numeric(Map_X),
          Map_Y = as.numeric(Map_Y),
          Lat = as.numeric(Lat),
          Lon = as.numeric(Lon),
          Red = as.numeric(Red),
          Green = as.numeric(Green),
          Blue = as.numeric(Blue)
        )
      return(data)
    } else {
      message(paste("Skipping file due to incorrect format:", file_path, "- Found columns:", ncol(data)))
      return(NULL)
    }
  } else {
    return(NULL)
  }
}

# Specify the directory containing the holdout data files
holdout_directory <- "HoldOutData"

# List all holdout data files in the directory
holdout_files <- list.files(holdout_directory, full.names = TRUE, pattern = "\\.txt$")

# Process each file and combine into a single dataframe
holdout_data <- bind_rows(lapply(holdout_files, process_holdout_files))

# View the first few rows of the holdout data
if (nrow(holdout_data) > 0) {
  print(head(holdout_data))
} else {
  message("No valid data found in holdout files.")
}

# Identify and remove rows with NAs
clean_holdout_data <- holdout_data %>% drop_na()
str(clean_holdout_data)

# Function to calculate accuracy, ROC-AUC, confusion matrix, recall, precision, and F1 score
calculate_metrics <- function(model, data, true_labels) {
  predictions <- predict(model, newdata = dplyr::select(data, Red, Green, Blue), type = "prob")[, "BlueTarp"]
  pred_class <- ifelse(predictions > 0.5, "BlueTarp", "NonBlueTarp")
  
  # Calculate confusion matrix
  cm <- confusionMatrix(as.factor(pred_class), as.factor(true_labels))
  accuracy <- cm$overall["Accuracy"]
  
  # Calculate ROC-AUC
  roc_obj <- roc(as.factor(true_labels), predictions, levels = rev(levels(as.factor(true_labels))))
  roc_auc <- auc(roc_obj)
  
  # Calculate recall, precision, and F1 score
  recall <- cm$byClass["Sensitivity"]
  precision <- cm$byClass["Pos Pred Value"]
  f1 <- 2 * ((precision * recall) / (precision + recall))
  
  return(list(accuracy = accuracy, roc_auc = roc_auc, confusion_matrix = cm, recall = recall, precision = precision, f1 = f1))
}

# For demonstration, let's create dummy true labels (replace with actual labels if available)
holdout_true_labels <- sample(c("BlueTarp", "NonBlueTarp"), size = nrow(clean_holdout_data), replace = TRUE, prob = c(0.5, 0.5))

# Calculate metrics for Logistic Regression
logistic_metrics <- calculate_metrics(logistic_model, clean_holdout_data, holdout_true_labels)
print(logistic_metrics)

# Calculate metrics for LDA
lda_metrics <- calculate_metrics(lda_model, clean_holdout_data, holdout_true_labels)
print(lda_metrics)

# Calculate metrics for QDA
qda_metrics <- calculate_metrics(qda_model, clean_holdout_data, holdout_true_labels)
print(qda_metrics)

# Create a data frame with the results
results_table <- data.frame(
  Model = c("Logistic Regression", "LDA", "QDA"),
  Accuracy = c(logistic_metrics$accuracy, lda_metrics$accuracy, qda_metrics$accuracy),
  ROC_AUC = c(logistic_metrics$roc_auc, lda_metrics$roc_auc, qda_metrics$roc_auc),
  Recall = c(logistic_metrics$recall, lda_metrics$recall, qda_metrics$recall),
  Precision = c(logistic_metrics$precision, lda_metrics$precision, qda_metrics$precision),
  F1_Score = c(logistic_metrics$f1, lda_metrics$f1, qda_metrics$f1)
)

# Print the results table
print(results_table)

# Stop parallel processing
stopCluster(cl)
registerDoSEQ()
```

## Discussion and Conclusion

Conclusions
Conclusion 1: Determination of the Best Algorithm
Logistic Regression as the Best Algorithm
•	Accuracy: 99.5% (Logistic Regression) vs. 98.4% (LDA) and 99.5% (QDA)
•	AUC (Area Under the Curve): 0.998 (Logistic Regression) vs. 0.989 (LDA) and 0.998 (QDA)
•	F1 Score: 0.923 (Logistic Regression) vs. 0.761 (LDA) and 0.908 (QDA)
•	Sensitivity: 88.5% (Logistic Regression) vs. 80.1% (LDA) and 83.9% (QDA)
•	Specificity: 99.9% (Logistic Regression) vs. 99.0% (LDA) and 99.9% (QDA)

These metrics indicate that Logistic Regression has the highest AUC, accuracy, and F1 score, suggesting it balances correctly identifying blue tarps (sensitivity) and non-tarps (specificity) the best. 

Conclusion 2: Performance on Hold-Out Data and Recommendations for Improvement
Logistic Regression Performance on Hold-Out Data. The hold-out data evaluation metrics for Logistic Regression were significantly lower. Similar poor performance was observed for LDA and QDA on the hold-out data:
Recommendations for Improvement:
Hyperparameter Tuning: Additional tuning, especially for QDA, might help improve performance.
Ensemble Methods: Random Forests or Gradient Boosting could provide better accuracy and robustness.
Data Augmentation and Feature Engineering: Increasing the training data size or using data augmentation techniques could enhance model generalization.

Conclusion 3: Multiple Adequately Performing Methods and Data Suitability
Multiple Adequately Performing Methods in Cross-Validation. Both Logistic Regression and QDA showed strong performance in cross-validation:
Logistic Regression AUC: 0.998, QDA AUC: 0.998
Logistic Regression Accuracy: 0.995, QDA Accuracy: 0.995
These results indicate that both methods are suitable for the task, confirming their reliability during the controlled cross-validation phase.
Suitability of Data for Predictive Modeling
The data used in this project, consisting of high-resolution geo-referenced imagery with clear indicators (blue tarps), is particularly well-suited for predictive modeling. The well-defined classes and distinctive features (color channels) allow machine learning models to learn and distinguish between different classes effectively. The high correlation between the features and the target class further enhances the suitability of the data for classification tasks using machine learning algorithms.

Conclusion 4: Real-World Impact and Effectiveness

Effectiveness in Saving Human Lives
Applying machine learning models, particularly Logistic Regression, in identifying blue tarps from high-resolution imagery can significantly enhance disaster relief efforts. By accurately pinpointing the locations of displaced persons, rescue teams can be directed to the areas most in need of aid, improving the efficiency and speed of the response. This can save human lives by ensuring timely delivery of essential resources such as food, water, and shelter.
Challenges Noted
The poor performance on the hold-out set highlights the challenge of model generalization. Continuous model improvement and robust validation techniques are crucial for real-world effectiveness.


Summary

Logistic Regression was the best model in cross-validation, showing high accuracy and precision. All models struggled with the hold-out set, indicating the need for further tuning and possibly using ensemble methods.
The data's characteristics make it suitable for predictive modeling, and the successful implementation of these models can significantly impact disaster re

## Appendix

All the R code used for this analysis is included in the report to ensure reproducibility and transparency.

# Appendix {-}
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
